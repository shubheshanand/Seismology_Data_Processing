{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLIO05frg0uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaTmJlU0i47D",
        "colab_type": "code",
        "outputId": "b757e6fc-4fec-451f-8128-af572b8a8b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJds_Stki7ha",
        "colab_type": "code",
        "outputId": "80dd7b53-7728-4d74-bdbd-34fc5cc8fba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/Seismology.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Seismology.zip\n",
            "replace Seismology/CNN.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace Seismology/image_loader.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSp0Jb2Kjs_I",
        "colab_type": "code",
        "outputId": "f93fa479-992c-4dc8-a5e1-fe367f88a754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Sampler\n",
        "from torchvision import utils\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "from torch.optim import Adam\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models, transforms\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "num_epochs = 100\n",
        "num_classes = 2\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "num_of_workers = 0\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train_loader: 301\n",
            "Noise Noise Seismic Seismic\n",
            "[1,   100] loss: 11.496\n",
            "[1,   200] loss: 10.479\n",
            "[1,   300] loss: 9.967\n",
            "[2,   100] loss: 8.828\n",
            "[2,   200] loss: 8.261\n",
            "[2,   300] loss: 7.185\n",
            "[3,   100] loss: 6.408\n",
            "[3,   200] loss: 5.599\n",
            "[3,   300] loss: 5.324\n",
            "[4,   100] loss: 4.835\n",
            "[4,   200] loss: 4.103\n",
            "[4,   300] loss: 3.885\n",
            "[5,   100] loss: 3.516\n",
            "[5,   200] loss: 3.341\n",
            "[5,   300] loss: 3.075\n",
            "[6,   100] loss: 2.992\n",
            "[6,   200] loss: 2.313\n",
            "[6,   300] loss: 2.774\n",
            "[7,   100] loss: 2.746\n",
            "[7,   200] loss: 2.549\n",
            "[7,   300] loss: 2.221\n",
            "[8,   100] loss: 1.996\n",
            "[8,   200] loss: 1.765\n",
            "[8,   300] loss: 2.152\n",
            "[9,   100] loss: 2.289\n",
            "[9,   200] loss: 1.853\n",
            "[9,   300] loss: 1.495\n",
            "[10,   100] loss: 1.407\n",
            "[10,   200] loss: 1.785\n",
            "[10,   300] loss: 1.593\n",
            "[11,   100] loss: 1.673\n",
            "[11,   200] loss: 1.337\n",
            "[11,   300] loss: 1.220\n",
            "[12,   100] loss: 1.388\n",
            "[12,   200] loss: 1.162\n",
            "[12,   300] loss: 1.109\n",
            "[13,   100] loss: 1.088\n",
            "[13,   200] loss: 1.464\n",
            "[13,   300] loss: 1.299\n",
            "[14,   100] loss: 1.393\n",
            "[14,   200] loss: 1.216\n",
            "[14,   300] loss: 0.693\n",
            "[15,   100] loss: 1.358\n",
            "[15,   200] loss: 1.009\n",
            "[15,   300] loss: 1.141\n",
            "[16,   100] loss: 1.037\n",
            "[16,   200] loss: 0.926\n",
            "[16,   300] loss: 0.962\n",
            "[17,   100] loss: 1.163\n",
            "[17,   200] loss: 1.000\n",
            "[17,   300] loss: 1.029\n",
            "[18,   100] loss: 1.118\n",
            "[18,   200] loss: 0.980\n",
            "[18,   300] loss: 1.176\n",
            "[19,   100] loss: 1.239\n",
            "[19,   200] loss: 0.668\n",
            "[19,   300] loss: 0.725\n",
            "[20,   100] loss: 0.857\n",
            "[20,   200] loss: 1.711\n",
            "[20,   300] loss: 0.899\n",
            "[21,   100] loss: 1.000\n",
            "[21,   200] loss: 0.789\n",
            "[21,   300] loss: 0.856\n",
            "[22,   100] loss: 1.044\n",
            "[22,   200] loss: 0.936\n",
            "[22,   300] loss: 0.997\n",
            "[23,   100] loss: 1.152\n",
            "[23,   200] loss: 0.925\n",
            "[23,   300] loss: 1.236\n",
            "[24,   100] loss: 0.755\n",
            "[24,   200] loss: 0.850\n",
            "[24,   300] loss: 1.164\n",
            "[25,   100] loss: 0.964\n",
            "[25,   200] loss: 0.751\n",
            "[25,   300] loss: 1.029\n",
            "[26,   100] loss: 1.125\n",
            "[26,   200] loss: 1.078\n",
            "[26,   300] loss: 0.894\n",
            "[27,   100] loss: 0.860\n",
            "[27,   200] loss: 1.082\n",
            "[27,   300] loss: 0.701\n",
            "[28,   100] loss: 1.089\n",
            "[28,   200] loss: 0.901\n",
            "[28,   300] loss: 0.876\n",
            "[29,   100] loss: 0.781\n",
            "[29,   200] loss: 0.889\n",
            "[29,   300] loss: 0.886\n",
            "[30,   100] loss: 0.681\n",
            "[30,   200] loss: 0.686\n",
            "[30,   300] loss: 0.789\n",
            "[31,   100] loss: 0.997\n",
            "[31,   200] loss: 0.906\n",
            "[31,   300] loss: 0.720\n",
            "[32,   100] loss: 0.678\n",
            "[32,   200] loss: 0.752\n",
            "[32,   300] loss: 0.649\n",
            "[33,   100] loss: 0.808\n",
            "[33,   200] loss: 0.809\n",
            "[33,   300] loss: 0.825\n",
            "[34,   100] loss: 0.724\n",
            "[34,   200] loss: 1.003\n",
            "[34,   300] loss: 0.747\n",
            "[35,   100] loss: 0.378\n",
            "[35,   200] loss: 1.060\n",
            "[35,   300] loss: 1.010\n",
            "[36,   100] loss: 0.693\n",
            "[36,   200] loss: 0.738\n",
            "[36,   300] loss: 0.733\n",
            "[37,   100] loss: 0.770\n",
            "[37,   200] loss: 0.909\n",
            "[37,   300] loss: 0.472\n",
            "[38,   100] loss: 0.670\n",
            "[38,   200] loss: 0.522\n",
            "[38,   300] loss: 0.680\n",
            "[39,   100] loss: 0.756\n",
            "[39,   200] loss: 0.778\n",
            "[39,   300] loss: 0.793\n",
            "[40,   100] loss: 0.868\n",
            "[40,   200] loss: 0.484\n",
            "[40,   300] loss: 0.897\n",
            "[41,   100] loss: 0.430\n",
            "[41,   200] loss: 0.683\n",
            "[41,   300] loss: 0.666\n",
            "[42,   100] loss: 0.802\n",
            "[42,   200] loss: 0.621\n",
            "[42,   300] loss: 0.969\n",
            "[43,   100] loss: 0.604\n",
            "[43,   200] loss: 0.933\n",
            "[43,   300] loss: 0.479\n",
            "[44,   100] loss: 0.643\n",
            "[44,   200] loss: 0.680\n",
            "[44,   300] loss: 0.401\n",
            "[45,   100] loss: 0.526\n",
            "[45,   200] loss: 0.827\n",
            "[45,   300] loss: 0.673\n",
            "[46,   100] loss: 0.591\n",
            "[46,   200] loss: 0.614\n",
            "[46,   300] loss: 0.659\n",
            "[47,   100] loss: 0.798\n",
            "[47,   200] loss: 0.419\n",
            "[47,   300] loss: 0.596\n",
            "[48,   100] loss: 0.524\n",
            "[48,   200] loss: 0.585\n",
            "[48,   300] loss: 0.574\n",
            "[49,   100] loss: 0.465\n",
            "[49,   200] loss: 0.564\n",
            "[49,   300] loss: 0.325\n",
            "[50,   100] loss: 0.847\n",
            "[50,   200] loss: 0.632\n",
            "[50,   300] loss: 0.525\n",
            "[51,   100] loss: 0.366\n",
            "[51,   200] loss: 0.554\n",
            "[51,   300] loss: 0.492\n",
            "[52,   100] loss: 0.598\n",
            "[52,   200] loss: 0.368\n",
            "[52,   300] loss: 0.639\n",
            "[53,   100] loss: 0.701\n",
            "[53,   200] loss: 0.507\n",
            "[53,   300] loss: 0.743\n",
            "[54,   100] loss: 0.554\n",
            "[54,   200] loss: 0.441\n",
            "[54,   300] loss: 0.571\n",
            "[55,   100] loss: 0.612\n",
            "[55,   200] loss: 0.788\n",
            "[55,   300] loss: 0.445\n",
            "[56,   100] loss: 0.401\n",
            "[56,   200] loss: 0.457\n",
            "[56,   300] loss: 0.475\n",
            "[57,   100] loss: 0.488\n",
            "[57,   200] loss: 0.578\n",
            "[57,   300] loss: 0.420\n",
            "[58,   100] loss: 0.445\n",
            "[58,   200] loss: 0.675\n",
            "[58,   300] loss: 0.556\n",
            "[59,   100] loss: 0.848\n",
            "[59,   200] loss: 0.636\n",
            "[59,   300] loss: 0.261\n",
            "[60,   100] loss: 0.502\n",
            "[60,   200] loss: 0.466\n",
            "[60,   300] loss: 0.495\n",
            "[61,   100] loss: 0.545\n",
            "[61,   200] loss: 0.300\n",
            "[61,   300] loss: 0.535\n",
            "[62,   100] loss: 0.448\n",
            "[62,   200] loss: 0.551\n",
            "[62,   300] loss: 0.467\n",
            "[63,   100] loss: 0.425\n",
            "[63,   200] loss: 0.440\n",
            "[63,   300] loss: 0.397\n",
            "[64,   100] loss: 0.236\n",
            "[64,   200] loss: 0.330\n",
            "[64,   300] loss: 0.457\n",
            "[65,   100] loss: 0.545\n",
            "[65,   200] loss: 0.276\n",
            "[65,   300] loss: 0.558\n",
            "[66,   100] loss: 0.534\n",
            "[66,   200] loss: 0.405\n",
            "[66,   300] loss: 0.584\n",
            "[67,   100] loss: 0.469\n",
            "[67,   200] loss: 0.563\n",
            "[67,   300] loss: 0.533\n",
            "[68,   100] loss: 0.469\n",
            "[68,   200] loss: 0.460\n",
            "[68,   300] loss: 0.563\n",
            "[69,   100] loss: 0.329\n",
            "[69,   200] loss: 0.488\n",
            "[69,   300] loss: 0.278\n",
            "[70,   100] loss: 0.325\n",
            "[70,   200] loss: 0.540\n",
            "[70,   300] loss: 0.341\n",
            "[71,   100] loss: 0.336\n",
            "[71,   200] loss: 0.628\n",
            "[71,   300] loss: 0.628\n",
            "[72,   100] loss: 0.346\n",
            "[72,   200] loss: 0.571\n",
            "[72,   300] loss: 0.459\n",
            "[73,   100] loss: 0.224\n",
            "[73,   200] loss: 0.479\n",
            "[73,   300] loss: 0.390\n",
            "[74,   100] loss: 0.431\n",
            "[74,   200] loss: 0.336\n",
            "[74,   300] loss: 0.454\n",
            "[75,   100] loss: 0.534\n",
            "[75,   200] loss: 0.462\n",
            "[75,   300] loss: 0.330\n",
            "[76,   100] loss: 0.405\n",
            "[76,   200] loss: 0.462\n",
            "[76,   300] loss: 0.348\n",
            "[77,   100] loss: 0.423\n",
            "[77,   200] loss: 0.483\n",
            "[77,   300] loss: 0.543\n",
            "[78,   100] loss: 0.382\n",
            "[78,   200] loss: 0.433\n",
            "[78,   300] loss: 0.435\n",
            "[79,   100] loss: 0.393\n",
            "[79,   200] loss: 0.308\n",
            "[79,   300] loss: 0.360\n",
            "[80,   100] loss: 0.282\n",
            "[80,   200] loss: 0.425\n",
            "[80,   300] loss: 0.526\n",
            "[81,   100] loss: 0.314\n",
            "[81,   200] loss: 0.312\n",
            "[81,   300] loss: 0.390\n",
            "[82,   100] loss: 0.268\n",
            "[82,   200] loss: 0.481\n",
            "[82,   300] loss: 0.327\n",
            "[83,   100] loss: 0.211\n",
            "[83,   200] loss: 0.260\n",
            "[83,   300] loss: 0.271\n",
            "[84,   100] loss: 0.418\n",
            "[84,   200] loss: 0.386\n",
            "[84,   300] loss: 0.246\n",
            "[85,   100] loss: 0.465\n",
            "[85,   200] loss: 0.378\n",
            "[85,   300] loss: 0.381\n",
            "[86,   100] loss: 0.453\n",
            "[86,   200] loss: 0.438\n",
            "[86,   300] loss: 0.295\n",
            "[87,   100] loss: 0.392\n",
            "[87,   200] loss: 0.345\n",
            "[87,   300] loss: 0.421\n",
            "[88,   100] loss: 0.187\n",
            "[88,   200] loss: 0.440\n",
            "[88,   300] loss: 0.445\n",
            "[89,   100] loss: 0.250\n",
            "[89,   200] loss: 0.238\n",
            "[89,   300] loss: 0.389\n",
            "[90,   100] loss: 0.345\n",
            "[90,   200] loss: 0.471\n",
            "[90,   300] loss: 0.423\n",
            "[91,   100] loss: 0.325\n",
            "[91,   200] loss: 0.370\n",
            "[91,   300] loss: 0.363\n",
            "[92,   100] loss: 0.342\n",
            "[92,   200] loss: 0.336\n",
            "[92,   300] loss: 0.390\n",
            "[93,   100] loss: 0.300\n",
            "[93,   200] loss: 0.263\n",
            "[93,   300] loss: 0.328\n",
            "[94,   100] loss: 0.330\n",
            "[94,   200] loss: 0.368\n",
            "[94,   300] loss: 0.401\n",
            "[95,   100] loss: 0.307\n",
            "[95,   200] loss: 0.391\n",
            "[95,   300] loss: 0.290\n",
            "[96,   100] loss: 0.311\n",
            "[96,   200] loss: 0.309\n",
            "[96,   300] loss: 0.313\n",
            "[97,   100] loss: 0.234\n",
            "[97,   200] loss: 0.326\n",
            "[97,   300] loss: 0.211\n",
            "[98,   100] loss: 0.390\n",
            "[98,   200] loss: 0.371\n",
            "[98,   300] loss: 0.318\n",
            "[99,   100] loss: 0.223\n",
            "[99,   200] loss: 0.342\n",
            "[99,   300] loss: 0.330\n",
            "[100,   100] loss: 0.295\n",
            "[100,   200] loss: 0.228\n",
            "[100,   300] loss: 0.349\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GroundTruth:  Seismic Seismic Seismic Seismic\n",
            "Predicted:  Seismic Noise Noise Seismic\n",
            "Accuracy of the network on the test images: 52 %\n",
            "Accuracy of Seismic : 45 %\n",
            "Accuracy of Noise : 56 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQjQENehCCNEQ1KALIURDUIMuhBAN4f8BjPuekZYd8y8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR0Rz_CabgRb",
        "colab_type": "code",
        "outputId": "5faf29e2-4db4-45ae-c611-11c99184bdee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "DATA_PATH_TRAIN = '/content/Seismology/Train/Spectrogram'\n",
        "DATA_PATH_TEST = '/content/Seismology/Test'\n",
        "MODEL_STORE_PATH = '/content/Seismology'\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "# Seismology dataset\n",
        "train_dataset = datasets.ImageFolder(root=DATA_PATH_TRAIN, transform=trans)\n",
        "test_dataset = datasets.ImageFolder(root=DATA_PATH_TEST, transform=trans)\n",
        "\n",
        "# Create custom random sampler class to iter over dataloader.\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_of_workers)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_of_workers)\n",
        "\n",
        "classes = ('Seismic', 'Noise')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    # plt.imshow((npimg* 255).astype(np.uint8))\n",
        "\n",
        "print('Train_loader:', len(train_loader))\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train_loader: 301\n",
            "Noise Seismic Seismic Seismic\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhDKEMXEWkIZegiIg3x/0yVRsuM8NYYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODWuBXZrGrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net1 = models.resnet18(pretrained=True)\n",
        "num_ftrs = net1.fc.in_features\n",
        "net1.fc1 = nn.Linear(num_ftrs,2)\n",
        "net1 = net1.to(device)\n",
        "\n",
        "net2 = models.resnet18(pretrained=True)\n",
        "num_ftrs = net2.fc.in_features\n",
        "net2.fc1 = nn.Linear(num_ftrs,2)\n",
        "net2 = net2.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.cuda()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9, weight_decay=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGG0KGNPrOml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = data[0].to(device)\n",
        "        labels = data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print('a')\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n",
        "\n",
        "images = images.to(device)\n",
        "outputs = net(images)\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size)))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "class_correct = list(0. for i in range(2))\n",
        "class_total = list(0. for i in range(2))\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(2):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(2):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}